{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a32d09c23c12b77",
   "metadata": {},
   "source": [
    "# From Graphs to Probability Distributions: A Hands-On Introduction to ERGMs\n",
    "#### By Tom Talpir (tom.talpir@weizmann.ac.il)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc42ea897230e4",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98746fbe186115ef",
   "metadata": {},
   "source": [
    "Graphs are extremely useful objects for describing many complex systems. From molecules and neural networks in the brain to social networks, graphs provide a natural way to represent interactions between components and capture both local and global structure. Understanding the structural properties of these graphs, and the design principles that govern their connectivity, is therefore crucial for understanding the phenomena they describe.\n",
    "\n",
    "For example, suppose we are given the connectome of a worm and are interested in studying the rules and patterns underlying its neuronal connectivity. One approach would be to perform a detailed analysis of this specific observed graph in isolation, but such an analysis would inevitably be biased toward this particular sample. Instead, we can take a statistical approach and build a model that assigns a probability to every possible graph as a function of a set of structural features. Such a model allows us to ask which structural features are truly important for explaining the observed connectivity. Intuitively, we would expect structural properties that play a key role in shaping the connectivity to be associated with stronger model parameters, while irrelevant properties should have little or no effect.\n",
    "\n",
    "Exponential Random Graph Models (**ERGMs**) are a class of statistical models designed for exactly this purpose. An ERGM defines a probability distribution over graphs, where the likelihood of a graph depends on a set of chosen structural features (such as the number of reciprocal connections), weighted by model parameters that quantify their importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e85523695889d4",
   "metadata": {},
   "source": [
    "### Tutorial goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9aa41e089445d",
   "metadata": {},
   "source": [
    "This tutorial has two main learning goals. We will fit an ERGM to observed network data by finding the optimal model parameters. In doing so, we will introduce and use two important statistical tools:\n",
    "\n",
    "* Maximum Likelihood Estimation (**MLE**) via **gradient descent**\n",
    "* Markov Chain Monte Carlo (**MCMC**) simulations\n",
    "\n",
    "We will demonstrate these ideas using two example graphs:\n",
    "* **Sampson’s monastery network** — a small graph consisting of 18 nodes, used as an initial toy example. Each node represents a monk in a New England monastery, and a directed edge indicates the presence of a friendly relationship between two monks.\n",
    "* **C. elegans connectome** — a graph consisting of 279 nodes, representing the sensory neurons, interneurons, and motor neurons in the nervous system of *C. elegans*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb5745701ff1d9",
   "metadata": {},
   "source": [
    "### Step 1 - What is a graph and an ERGM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabbf2b",
   "metadata": {},
   "source": [
    "Formally, a graph $G$ consists of $n$ nodes that are connected to one another by a set of edges. The connectivity is represented by a binary connectivity matrix $W$, where $W_{i,j} = 1$ if nodes $i$ and $j$ are connected, and $0$ otherwise. The graph can be **directed**, meaning that edges have a direction, or **undirected**, in which case $W_{i,j} = W_{j,i}$.\n",
    "\n",
    "An ERGM defines a random variable $\\mathbf{Y}$, which represents a random graph on $n$ nodes. The probability of observing a specific graph $y$ is given by\n",
    "$$\n",
    "\\Pr(\\mathbf{Y}=y \\mid \\theta) = \\frac{\\exp(\\theta^T g(y))}{\\sum_{z \\in \\mathcal{Y}} \\exp(\\theta^T g(z))},\n",
    "$$\n",
    "where $\\mathcal{Y}$ is the set of all graphs on $n$ nodes, $g(y)$ is a vector of graph statistics describing $y$, and $\\theta \\in \\mathbb{R}^q$ is a vector of model parameters.\n",
    "\n",
    "In essence, an ERGM defines a probability distribution over all possible graphs on $n$ nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ea5e51802bbeb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./Haber_ERGM_illustration.png\" width=\"300\">\n",
    "  <p style=\"font-size:12px;\"><em>(Haber et al., 2023)</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44998fc74f9125b0",
   "metadata": {},
   "source": [
    "### Step 2 - Getting to know the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da51be900f862a",
   "metadata": {},
   "source": [
    "We begin by loading the Sampson monastery graph, represented as an $18 \\times 18$ binary connectivity matrix. $W_{i,j}$ indicates that monk $i$ claimed to have a friendly relationship with monk $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a399b58a9712c166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:44:35.186763Z",
     "start_time": "2025-12-12T13:44:35.181822Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "W = np.array(\n",
    "    [[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
    "     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
    "     [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "     [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "     [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "     [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
    "     [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "     [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
    ")\n",
    "\n",
    "n_nodes = W.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100c6a9d519ce63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:04:23.457587Z",
     "start_time": "2025-12-11T13:04:23.450472Z"
    }
   },
   "source": [
    "We will use 2 structural features for our model -\n",
    "* Number of edges - how many relationships exist in the graph\n",
    "* Number of reciprocal edges - the number of reciprocal relationships, where $W_{i,j} = W_{j, i}$\n",
    "\n",
    "**[TASK]** In the next cell, complete the `get_number_of_edges` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf0e54d63f45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_edges(W: np.ndarray):\n",
    "    \"\"\"\n",
    "    Counts the number of edges in a directed graph\n",
    "\n",
    "    Parameters:\n",
    "         W: An n by n binary connectivity matrix\n",
    "\n",
    "    Returns:\n",
    "        Number of edges in the graph\n",
    "    \"\"\"\n",
    "    number_of_edges = None\n",
    "\n",
    "    #####################\n",
    "    ### COMPLETE THIS ###\n",
    "    #####################\n",
    "\n",
    "    return number_of_edges\n",
    "\n",
    "\n",
    "def get_number_of_reciprocal_edges(W: np.ndarray):\n",
    "    \"\"\"\n",
    "    Counts the number of reciprocal edges in a directed graph. Here we use a neat trick of calculating W^2 to find nodes that have a path of length 2\n",
    "\n",
    "    Parameters:\n",
    "         W: An n by n binary connectivity matrix\n",
    "\n",
    "    Returns:\n",
    "        Number of reciprocal edges in the graph\n",
    "    \"\"\"\n",
    "    return (W * W.T).sum() / 2\n",
    "\n",
    "def calculate_graph_statistics(W: np.ndarray, feature_calculators):\n",
    "    \"\"\"\n",
    "    Given a directed graph W and a list of callable feature calculators, we calculate the graph statistics.\n",
    "\n",
    "    Parameters:\n",
    "        W: Connectivity matrix for which the statistics should be calculated\n",
    "        feature_calculators: A list of callable feature calculators. Each feature calculator should be a callable object that receives a connectivity matrix, and returns the feature statistics.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the graph statistics.\n",
    "    \"\"\"\n",
    "    num_features = len(feature_calculators)\n",
    "    statistics = np.zeros(num_features)\n",
    "\n",
    "    for i, feature_calculator in enumerate(feature_calculators):\n",
    "        statistics[i] = feature_calculator(W)\n",
    "\n",
    "    return statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499453fbef3cfb0",
   "metadata": {},
   "source": [
    "Let's calculate the graph statistics using the 2 features we just defined.\n",
    "<br>\n",
    "**[TASK]** Verify that the graph has **88 edges** and **28 reciprocal edges**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f1da93bde6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_calculators = [get_number_of_edges, get_number_of_reciprocal_edges]\n",
    "observed_stats = calculate_graph_statistics(W, feature_calculators)\n",
    "print(f\"Number of edges - {observed_stats[0]}\")\n",
    "print(f\"Number of reciprocal edges - {observed_stats[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e815c25bd724f",
   "metadata": {},
   "source": [
    "### Step 3 - Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5c135",
   "metadata": {},
   "source": [
    "A common approach for estimating the parameters of a probability distribution is to maximize a likelihood function, such that under the assumed statistical model, the observed data is the most probable outcome. This procedure is known as Maximum Likelihood Estimation (**MLE**).\n",
    "\n",
    "The likelihood function assigns a probability to the observed data under different choices of model parameters. In our case, the likelihood can be written as\n",
    "$$\n",
    "\\mathcal{L}(\\theta \\mid y_{\\text{obs}}) = \\frac{\\exp(\\theta^T g(y_{\\text{obs}}))}{\\sum_{z \\in \\mathcal{Y}} \\exp(\\theta^T g(z))},\n",
    "$$\n",
    "where $y_{\\text{obs}}$ denotes the observed graph.\n",
    "\n",
    "In this toy example, we define two structural features: the number of edges and the number of reciprocal edges. Accordingly, $g(y)$ is a vector of two statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd72e5",
   "metadata": {},
   "source": [
    "It is common practice to work with the log-likelihood function instead, as it is more convenient to work with -\n",
    "$$\n",
    "\\ell (\\theta \\mid y_{\\text{obs}}) = \\theta^T g(y_{\\text{obs}}) - \\log\\!\\left(\\sum_{z \\in \\mathcal{Y}} \\exp(\\theta^T g(z))\\right)\n",
    "$$\n",
    "\n",
    "We now wish to find the model parameters that maximize the log-likelihood function, denoted by $\\theta^*$. This is equivalent to minimizing the negative log-likelihood, which is often more convenient since many optimization algorithms are formulated as minimization problems:\n",
    "$$\n",
    "\\theta^* = \\arg \\max \\ell (\\theta \\mid y_{\\text{obs}}) = \\arg \\min \\big(-\\ell (\\theta \\mid y_{\\text{obs}})\\big)\n",
    "$$\n",
    "\n",
    "We will start with the simplest optimization algorithm: **gradient descent**. Gradient descent is an iterative method for minimizing a differentiable function. At each step, the parameters are updated by moving in the opposite direction of the gradient, with the goal of converging to a minimum of the function. Formally, the update rule is given by\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\eta \\nabla \\ell(\\theta),\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271f622",
   "metadata": {},
   "source": [
    "We now need to compute the gradient of the log-likelihood function. It is perfectly fine to skip the derivation on a first read, but the underlying idea is relatively simple and, with a bit of patience, should become clear.\n",
    "\n",
    "The log-likelihood function can be differentiated with respect to $\\theta$ to obtain the gradient -\n",
    "$$\n",
    "\\nabla \\ell(\\theta) = \\frac{\\partial}{\\partial \\theta} \\ \\ell (\\theta) = g(y_{\\text{obs}}) - \\frac{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))g(z)}{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= g(y_{\\text{obs}}) - \\sum_{z\\in\\mathcal{Y}} \\frac{\\exp(\\theta^Tg(z))}{Z}g(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= g(y_{\\text{obs}})- \\sum_{z\\in\\mathcal{Y}}\\Pr_{\\theta, \\mathcal{Y}}(\\mathbf{Y}=z)g(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= g(y_{\\text{obs}})- \\mathbb{E}_{z\\sim\\mathcal{Y}}[g(z)]\n",
    "$$\n",
    "\n",
    "where $Z=\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))$ is the normalization factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb7fa66743bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:25:57.207786Z",
     "start_time": "2025-12-10T10:25:57.205365Z"
    }
   },
   "source": [
    "Notice that the gradient decomposes into two terms - the statistics of the observed graph, $g(y_{\\text{obs}})$, which we can compute directly, and the expected statistics over all possible graphs, $\\mathbb{E}_{z \\sim \\mathcal{Y}}[g(z)]$. Computing this expectation exactly is computationally infeasible, as it would require iterating over all directed graphs on $n$ nodes. Even for a relatively small graph such as the Sampson network, this corresponds to $2^{n^2 - n} = 2^{306}$ possible graphs.\n",
    "\n",
    "This is precisely where Markov Chain Monte Carlo (**MCMC**) simulations become useful, as we will see in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9eeb210dd4370",
   "metadata": {},
   "source": [
    "### Step 4 – MCMC simulations\n",
    "\n",
    "As we have just seen, we cannot directly compute $\\mathbb{E}_{z \\sim \\mathcal{Y}}[g(z)]$ due to the exponential number of possible graphs that would need to be considered. An alternative is to approximate this expectation by computing the sample mean of $g(z)$ over a collection of sampled graphs. This naturally raises the question - how should we choose this sample?\n",
    "\n",
    "One naive approach would be to generate graphs by independently flipping a coin with probability $p$ for each possible edge. However, this approach is too simplistic, and it is unlikely that such samples would faithfully represent the target distribution. We therefore need a more principled method, which is where Markov Chain Monte Carlo (**MCMC**) techniques come into play.\n",
    "\n",
    "MCMC refers to a class of algorithms used to draw samples from probability distributions. In this tutorial, we will focus on the Metropolis–Hastings algorithm, which is a widely used MCMC method.\n",
    "\n",
    "In its simplest form, the Metropolis–Hastings algorithm generates samples by iteratively performing the following steps -\n",
    "\n",
    "1. Start from an initial graph $y_0 \\in \\mathcal{Y}$ and set it as the current graph, $y_{\\text{current}} = y_0$.\n",
    "2. Propose a new graph $y_{\\text{proposed}}$ by making a small modification to $y_{\\text{current}}$ by adding or removing an edge between a randomly chosen pair of nodes $i, j$.\n",
    "3. Compute the *change score*, defined as $\\delta_g(y)_{i,j} = g(y_{\\text{proposed}}) - g(y_{\\text{current}})$.\n",
    "4. Accept the proposed graph with probability $p_{\\text{accept}} = \\min\\left(1, \\exp\\!\\left(\\theta^T \\delta_g(y)_{i,j}\\right)\\right)$.\n",
    "\n",
    "Steps 2–4 are repeated for a large number of iterations, until a sufficient number of graphs have been collected.\n",
    "\n",
    "Once we have obtained a sample of graphs, we can approximate the expectation $\\mathbb{E}_{z \\sim \\mathcal{Y}}[g(z)]$ using the sample mean.\n",
    "\n",
    "**[TASK]** Complete the following MCMC sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48cf827a7258e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T12:00:27.530410Z",
     "start_time": "2025-12-11T12:00:27.519706Z"
    }
   },
   "outputs": [],
   "source": [
    "def MCMC_graph_sample(thetas: np.ndarray,\n",
    "                      feature_calculators,\n",
    "                      n_nodes: int,\n",
    "                      num_graphs=1000,\n",
    "                      seed_graph=None,\n",
    "                      seed_graph_edge_probability=0.1,\n",
    "                      burn_in=1000,\n",
    "                      steps_per_sample=10\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Run an MCMC sampling process sample graphs from the current distribution.\n",
    "\n",
    "    Parameters:\n",
    "        thetas: current  distribution parameters, of size `# of features`.\n",
    "        feature_calculators: a list of callable feature calculators.\n",
    "        n_nodes: number of nodes in the graph.\n",
    "        num_graphs: number of graphs to sample.\n",
    "        seed_graph_edge_probability: Edge probability of the seed graph, generated from an Erdős–Rényi model.\n",
    "        burn_in: number of graphs that aren't collected to the final sample, as part of the burn in process.\n",
    "        steps_per_sample: number of edge flips before evaluating a candidate graph.\n",
    "\n",
    "    Returns:\n",
    "        A collection of graphs, represented as numpy arrays, sampled from the distribution.\n",
    "    \"\"\"\n",
    "    if seed_graph is None:\n",
    "        seed_graph = np.random.choice([0, 1], size=(n_nodes, n_nodes), p=[1-seed_graph_edge_probability, seed_graph_edge_probability])\n",
    "        seed_graph[np.diag_indices(n_nodes)] = 0\n",
    "\n",
    "    sampled_graphs = []\n",
    "\n",
    "    current_graph = seed_graph\n",
    "    current_statistics = calculate_graph_statistics(current_graph, feature_calculators)\n",
    "\n",
    "    while len(sampled_graphs) < num_graphs:\n",
    "        #####################\n",
    "        ### COMPLETE THIS ###\n",
    "        #####################\n",
    "        continue\n",
    "\n",
    "    return sampled_graphs\n",
    "\n",
    "def calculate_mean_statistics(sample_graphs: list, feature_calculators):\n",
    "    \"\"\"\n",
    "    Calculate the mean statistics over a collection of graphs.\n",
    "\n",
    "    Parameters:\n",
    "        sample_graphs: a list of graphs, each graph is a numpy array.\n",
    "        feature_calculators: a list of callable feature calculators.\n",
    "\n",
    "    Returns:\n",
    "        The mean statistics over the provided graphs, of size `# of features`.\n",
    "    \"\"\"\n",
    "    statistics = np.zeros((len(sample_graphs), len(feature_calculators)))\n",
    "\n",
    "    for i, W in enumerate(sample_graphs):\n",
    "        statistics[i] = calculate_graph_statistics(W, feature_calculators)\n",
    "\n",
    "    return np.mean(statistics, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d8f49",
   "metadata": {},
   "source": [
    "### Step 5 – Running the optimization loop\n",
    "\n",
    "We have now reached the final step - training the model. We start from the observed graph and an initial set of model parameters, and then iteratively apply the gradient descent update rule.\n",
    "\n",
    "**[TASK]** In the next cell, complete the gradient computation based on the derivation in the previous steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282a287b5c786fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T16:11:10.918261Z",
     "start_time": "2025-12-11T16:11:10.229767Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_calculators = [get_number_of_edges, get_number_of_reciprocal_edges]\n",
    "observed_stats = calculate_graph_statistics(W, feature_calculators)\n",
    "num_features = len(observed_stats)\n",
    "\n",
    "initial_thetas = np.random.uniform(-1, 1, num_features)\n",
    "num_opt_steps = 100\n",
    "learning_rate = 0.001\n",
    "MCMC_number_of_graphs_per_sample = 100\n",
    "MCMC_burn_in = 0\n",
    "MCMC_seed_graph_density = np.sum(W) / (n_nodes * (n_nodes-1))\n",
    "\n",
    "seed_graph = np.random.choice([0, 1], size=(n_nodes, n_nodes), p=[1-MCMC_seed_graph_density, MCMC_seed_graph_density])\n",
    "seed_graph[np.diag_indices(n_nodes)] = 0\n",
    "\n",
    "print(f\"Observed statistics - {observed_stats}\")\n",
    "print(f\"Seed MCMC graph density - {MCMC_seed_graph_density}\")\n",
    "print(f\"Initial thetas: {initial_thetas}\")\n",
    "gradients = []\n",
    "thetas = initial_thetas\n",
    "\n",
    "all_graphs = []\n",
    "all_thetas = []\n",
    "for i in range(num_opt_steps):\n",
    "    all_thetas.append(thetas)\n",
    "    print(f\"Working on step {i+1}/{num_opt_steps}\")\n",
    "\n",
    "    if i == 0:\n",
    "        burn_in = MCMC_burn_in\n",
    "    else:\n",
    "        burn_in = 0\n",
    "\n",
    "    sample_graphs = MCMC_graph_sample(\n",
    "        thetas=thetas,\n",
    "        feature_calculators=feature_calculators,\n",
    "        n_nodes=n_nodes,\n",
    "        num_graphs=MCMC_number_of_graphs_per_sample,\n",
    "        seed_graph=seed_graph,\n",
    "        burn_in=burn_in,\n",
    "        steps_per_sample=5\n",
    "    )\n",
    "\n",
    "    print(f\"\\tSampled {len(sample_graphs)} graphs\")\n",
    "\n",
    "    sample_statistics = calculate_mean_statistics(sample_graphs, feature_calculators)\n",
    "    print(f\"\\tMean statistics: {sample_statistics}\")\n",
    "\n",
    "    gradient = None\n",
    "    #####################\n",
    "    ### COMPLETE THIS ###\n",
    "    #####################\n",
    "\n",
    "    gradients.append(gradient)\n",
    "    print(f\"\\tGradient: {gradient}\")\n",
    "\n",
    "    thetas = thetas - learning_rate * gradient\n",
    "\n",
    "    # Use the last sampled graph for the next starting point of the MCMC chain\n",
    "    seed_graph = sample_graphs[-1]\n",
    "\n",
    "    all_graphs.extend(sample_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae451c00",
   "metadata": {},
   "source": [
    "### Step 6 – Evaluating the model\n",
    "\n",
    "After training the model, we want to evaluate its performance. Run the following cell, which should generate three figures -\n",
    "\n",
    "* Gradients over training iterations  \n",
    "* Number of edges across MCMC-sampled graphs  \n",
    "* Model parameters over training iterations  \n",
    "\n",
    "**[TASK]** Analyze each figure and try to answer the following questions - \n",
    "\n",
    "* How would you expect the gradient to behave as the optimization progresses?\n",
    "* Explain the behavior of the second figure (number of edges across the MCMC-sampled graphs). Why does it oscillate? Around what value is it oscillating? Is there anything special about that value?\n",
    "* Are the model parameters stable? Do they appear to converge to a specific value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a279bf39c11641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T16:13:25.506493Z",
     "start_time": "2025-12-11T16:13:25.384235Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 3), dpi=200)\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "ax[0].plot(np.mean(gradients, axis=1))\n",
    "ax[0].set_title(\"Gradient over time\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Gradient\")\n",
    "\n",
    "ax[1].plot([np.sum(g) for g in all_graphs])\n",
    "ax[1].set_title(\"# edges across MCMC graphs\")\n",
    "ax[1].set_xlabel(\"Number of graphs\")\n",
    "ax[1].set_ylabel(\"# edges\")\n",
    "\n",
    "ax[2].plot(all_thetas)\n",
    "ax[2].set_title(\"Model parameters\")\n",
    "ax[2].set_xlabel(\"Epoch\")\n",
    "ax[2].set_ylabel(\"Theta\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a96f08",
   "metadata": {},
   "source": [
    "### Step 7 – Train an ERGM model for the worm connectome\n",
    "\n",
    "Our final step is to repeat everything we have learned on a more interesting network - the *C. elegans* connectome.\n",
    "\n",
    "**[TASK]** Recreate the training loop from the previous steps, but this time for the worm connectome. This is a much larger network, so do not expect good results on the first attempt. You will likely need to tune hyperparameters such as the MCMC settings, the learning rate, and the number of optimization iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "worm_path = \"./worm.npy\"\n",
    "\n",
    "W = np.load(worm_path)\n",
    "n_nodes = W.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebcdd7",
   "metadata": {},
   "source": [
    "### Step 8 – Future tutorial ideas and directions\n",
    "\n",
    "* **Improved MCMC sampling** – MCMC offers more advanced techniques that can substantially improve sampling efficiency and lead to better mixing.\n",
    "* **Improved optimization** – Our current optimization approach is very basic, relying on vanilla gradient descent. Possible extensions include:\n",
    "  * More advanced algorithms such as Newton–Raphson\n",
    "  * Better initialization of the parameters $\\theta$ (production-level ERGM implementations often use an approach known as MPLE, which includes a neat trick based on logistic regression)\n",
    "* **Convergence criteria** – In the current implementation, training stops after a fixed number of iterations. A more robust approach would be to continue training until convergence is detected. Identifying convergence is a nontrivial problem and may require the use of more sophisticated statistical tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
